# 习题

## 习题1

>物理符号系统假说[1]是符号主义时代的理论基石。请分析：
>
>- 该假说的"充分性论断"和"必要性论断"分别是什么含义？
>- 结合本章内容，说明符号主义智能体在实践中遇到的哪些问题对该假说的"充分性"提出了挑战？
>- 大语言模型驱动的智能体是否符合物理符号系统假说？

1. Q1：
	1. 充分性论断：任何一个物理符号系统，都具备产生通用智能行为的充分手段
	2. 必要性论断：任何一个能够展现通用智能行为的系统，其本质必然是一个物理符号系统。
2. Q2：
	1. 难以构建一个能够支撑真实世界交互的知识库（知识获取瓶颈；常识问题）
	2. 难以处理动态变化的世界（如何高效判断哪些事物未发生改变；符号系统完全依赖预设规则，系统脆弱）
3. Q3：不符合。PSSH的核心概念是：“**符号**（像 “1+1=2” 里的数字和加号都是符号）”+“**对符号做操作**（比如把 “1+1” 改成 “2”，把 “苹果” 和 “红色” 组合成 “红色苹果”）”+“**通过搜索这些符号、按规则推理产生行为**（比如解数学题时，一步步套公式推导）”。单纯的LLM智能体的核心是 “靠数据学规律”，不是靠固定符号和规则。


## 习题2

>专家系统MYCIN[2]在医疗诊断领域取得了显著成功，但最终并未大规模应用于临床实践。请思考：
>
>@提示：可以从技术、伦理、法律、用户接受度等多个角度分析
>
>- 除了本章提到的"知识获取瓶颈"和"脆弱性"，还有哪些因素可能阻碍了专家系统在医疗等高风险领域的应用？
>- 如果让现在的你设计一个医疗诊断智能体，你会如何设计系统来克服MYCIN的局限？
>- 在哪些垂直领域中，基于规则的专家系统至今仍然是比深度学习更好的选择？请举例说明。

1. 灵活性与扩展性差，无法适配复杂临床背景。首先医疗知识本身存在大量模糊地带，不同医生的经验可能互相矛盾；其次病人往往也难以精确描述症状或者缺少部分症状描述，这与MYCIN“非黑即白”的规则匹配逻辑不合，容易判断失误。此外，系统集成难度高也是一个重要因素，医院里有电子病历、检验设备、影像系统等多个信息平台，而早期专家系统大多是 “孤岛式” 的，没法和这些系统无缝对接。
2. 系统设计（见下表）
3. 基于规则的专家系统更适合规则明确、数据稀缺、容错率极低的领域。比如工业设备的故障诊断或合同条款检查。

| 层级  | 核心功能  | 具体实现方式  |
| :------------: | :------------: | :------------: |
|  感知与交互 | 自然语言理解+多源数据整合  | 用LLM做接口，读取电子病历中的文本、数值、医疗影像；支持医生用口语化的方式提问  |
| 推理与决策  |符号规则推理 + LLM 模糊推理| 知识图谱（符号库）：标准化内容，处理明确共有的医疗知识；LLM：处理模糊 / 边缘情况：比如症状不典型的病例，LLM 可以结合海量病历数据给出 “可能性排序” |
| 验证与责任  |可解释性输出 + 合规性保障 | 输出透明推理链；内置隐私保护模块；加人工复核节点（明确 “医生为主、系统为辅” ）  |



## 习题3

>在2.2节中，我们实现了一个简化版的ELIZA聊天机器人。请在此基础上进行扩展实践：
>
>提示：这是一道动手实践题，建议实际编写代码
>
>- 为ELIZA添加3-5条新的规则，使其能够处理更多样化的对话场景（如谈论工作、学习、爱好等）
>- 实现一个简单的"上下文记忆"功能：让ELIZA能够记住用户在对话中提到的关键信息（如姓名、年龄、职业），并在后续对话中引用
>- 对比你扩展后的ELIZA与ChatGPT，列举至少3个维度上存在的本质差异
>- 为什么基于规则的方法在处理开放域对话时会遇到"组合爆炸"问题并且难以扩展维护？能否使用数学的方法来说明？

代码具体见*ELIZA.py*。

### 本质差异
| 差异  |  ELIZA | ChatGPT  |
| :------------: | :------------: | :------------: |
| 语言处理方式  | 处理的是语言的符号形式，而不是语言的意义  | 处理的是语言的意义，而不是符号形式  |
| 知识储备  |  没有任何内置知识，只是机械执行 |  有知识的模型，能理解和推理 |
| 能力边界 | 手动预设的有限状态（机械化执行预设逻辑）  | 近乎无限的状态空间，行为是概率性的 |


### 出现组合爆炸问题的原因
开放域对话的特点是用户的输入是无限的、不可预测的；而基于规则的办法本质上是针对每一条可能的用户输入编写对应的回应规则。当对话的基本元素增加时，可能的组合类型数量就会指数级或阶乘级增长。

假设要构建一个基于规则的开放域对话系统，用于处理某一对话场景：
-  $S={s_1, s_2, ..., s_n}$ 为该场景下的基本语义元素（比如时间，地点，人物，情绪等）
-  $k_i$ 为第i个语义元素可能取值的数量（比如情绪有5种）
-  $P={p_1, p_2, ..., p_m}$ 为句式集合（比如感叹句，疑问句，陈述句等），m为句式的数量

每一种「语义元素的组合 + 句式的组合」，都需要至少一条规则来匹配。可能的组合数量为：

$$C = m \times \[\prod_{i=1}^{n} k_i\] = m \times k_1 \times k_2 \times ... \times k_n $$

当n或者 $k_i$ 增大时，C会迅速增长。

### 代码遇到的问题
#### 1. 添加的规则在对话的时候没有发挥作用
添加的规则：
```python
r'I hate learning(.*)': [
	"Why would you feel so?",
	"Tell me more about your hatred on {0}.",
	"What do you think would make you feel better?"
],
```
但在对话过程中输入新规则中的话题（eg：“I hate learning Math”）时，ELIZA的回应还是都基于以下规则：
```python
r'.*': [
	"Please tell me more.",
	"Let's change focus a bit... Tell me about your family.",
	"Can you elaborate on that?"
]
```
**原因：**
代码中respond函数会严格按rules字典的定义顺序匹配，具体规则优先于宽泛规则，兜底规则最后；而我的新增规则原本是直接加在```r'.*'```之后的;```r'.*' ```能匹配任何字符串，所以它会优先匹配所有输入，导致后面的新增规则永远没有机会被检查到。

**解决方案：**
1. 正则匹配按字典插入顺序执行，具体规则必须放在通配符规则（```r'.*'```）前面
2. 匹配优先级：越具体的规则（如 ```I like (.*)```）越要放在前面，越宽泛的规则（如 ```.* work .*```）放在中间，兜底的通配符规则（```.*```）必须放在最后。如果没有按照具体规则和宽泛规则的先后顺序写，还是会有可能出现直接匹配到通配符规则的情况。

## 习题4
>马文·明斯基在"心智社会"理论[7]中提出了一个革命性的观点：智能源于大量简单智能体的协作，而非单一的完美系统。
>- 在图2.6"搭建积木塔"的例子中，如果 GRASP 智能体突然失效了，整个系统会发生什么？这种去中心化架构的优势和劣势是什么？
>- 将"心智社会"理论与现在的一些多智能体系统（如CAMEL-Workforce、MetaGPT、CrewAI）进行对比，它们之间存在哪些关联和不同之处？
>- 马文·明斯基认为智能体可以是"无心"的简单过程，然而现在的大语言模型和智能体往往都拥有强大的推理能力。这是否意味着"心智社会"理论在大语言模型时代不再适用了？

1. 无法完成抓取任务；因为抓取子任务没有完成，ADD-BLOCK任务也无法完成，整个智能体停滞在这一步。
	1. 优势：单一子智能体失效不会直接瘫痪整个系统，替换/修复即可；扩展灵活，无需对整个智能体进行重构；每个智能体只负责单一任务，降低了单个模块的复杂度。
	2. 劣势：高度依赖各个智能体的协作，一旦失效的是关键子智能体，容易导致任务停滞；难以从全局优化效率。
2. 关联：都将任务拆分为多个子任务，由不同智能体负责，并依赖智能体之间的协作；
	不同：现代多智能体基于llm，能够具备推理决策能力，并且是动态协作、非固定流程的。
3. 马文的理论重点是**将心智视为扁平化的、充满了互动与协作的“社会”**，不依赖于一个单一的原则。从这个角度看，心智社会理论仍然适用。

## 习题5
>强化学习与监督学习是两种不同的学习范式。请分析：
>
>- 用AlphaGo的例子说明强化学习的"试错学习"机制是如何工作的
>- 为什么强化学习特别适合序贯决策问题？它与监督学习在数据需求上有什么本质区别？
>- 现在我们需要训练一个会玩超级马里奥游戏的智能体。如果分别使用监督学习和强化学习，各需要什么数据？哪种方法对于这个任务来说更合适？
>- 在大语言模型的训练过程中，强化学习起到了什么关键性的作用？

1. h

## 习题6
>预训练-微调范式是现代人工智能领域的重要突破。请深入思考：
>- 为什么说预训练解决了符号主义时代的"知识获取瓶颈"问题？它们在知识表示方式上有什么本质区别？
>- 预训练模型的知识绝大部分来自互联网数据，这可能带来哪些问题？如何缓解以上问题？
>- 你认为"预训练-微调"范式是否可能会被某种新范式取代？或者它会长期存在？

## 习题7
>假设你要设计一个"智能代码审查助手"，它能够自动审查代码提交（Pull Request），概括代码的实现逻辑、检查代码质量、发现潜在BUG、提出改进建议。
>- 如果在符号主义时代（1980年代）设计这个系统，你会如何实现？会遇到什么困难？
>- 如果在没有大语言模型的深度学习时代（2015年左右），你会如何实现？
>- 在当前的大语言模型和智能体的时代，你会如何设计这个智能体的架构？它应该包含哪些模块（参考图2.10）？
>- 对比这三个时代的方案，说明智能体技术的演进如何使这个任务从"几乎不可能"变为"可行"
